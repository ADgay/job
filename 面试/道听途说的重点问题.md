#### 经典问题

##### 如何防止过拟合？

- 参数正则化（奥卡姆剃刀，减少模型自由度）
- 提前停止
- 数据增强
- BN层（可能是让数据分布相同，使局部特征不明显）
  - 均值归一化，使得各个维度上分布近似相同，一定程度上防止梯度下降时的抖动．[这里有图解](https://www.cnblogs.com/nolonely/p/6184196.html)
- dropout，类似集成学习？
- 决策树的剪枝操作

##### 类别不平衡问题？

- 欠采样
- 过采样，但需要特殊的过采样方法（SMOTE）
- 阈值移动
- 加上不同的损失权重.

##### 如何减小网络体积（韩松）

- 网络剪枝，将参数小于一定阈值的结点去掉／量化．
- mobile-net/shuffle-net．
- 蒸馏网络，fine-turning的感觉．

##### 手写几个代价函数

- 



##### BN,IN,LN,GN的区分

对数据进行白化操作，正好应对ｗ的零均值初始化．

- [BN](https://blog.csdn.net/xys430381_1/article/details/85141702)
  - 可以选择较大的学习率，使得训练速度增长很快，具有快速收敛性。（训练不会太受数据分布的影响）
  - 如果选择使用BN，使输入数据服从（０，１）分布，从而减少过拟合作用．
  - https://blog.csdn.net/xiaojiajia007/article/details/54924959(ｂｎ的反向传播)

##### 凸优化

可行域是凸集，目标函数是一个凸函数



##### 各种距离的描述

- 欧式距离，曼哈顿距离（L_P距离），（闵可夫斯基距离）
  $$
  L_p=(\sum|x_i^l-x_j^l|^p)^{\frac{1}{p}}
  $$
  p=1,曼哈顿距离．

  p=2,欧式距离

  p=正无穷，最大坐标差．切比雪夫距离．国际象棋中王移动到某个位置的最少步数．

- 海明距离

  两个编码之间的距离，码距．

- 马氏距离

  一个分布内，两个样本之间的距离．中间是分布的协方差矩阵．
  $$
  d_M(x,y)=\sqrt{(x-y)^T(\sum)^{-1}(x-y)}
  $$

- 余弦距离

  两个向量夹角的余弦值．

