#### 前言

##### 简单总结

机器学习覆盖范围很广，历史悠久，没有时间系统学习了，要有针对性地了解某些方法．比如面试常考SVM\随机森林，就多看一些；比如有的公司做推荐系统/有的做风控/有的做搜索，就需要具体了解一下涉及到哪些知识点．

主要还是把西瓜树过一遍．西瓜书上有很多实用过程中可能会遇到的问题,也有很多思路由来的解释,具体的算法也给出了引用,是一本非常好的入门教材.

##### 这里是一些参考与经验总结

[CITE1_XYZH的文章](https://www.zhihu.com/question/26726794/answer/151282052)

[机器之心](https://zhuanlan.zhihu.com/p/25327755)

[南瓜书](<https://github.com/datawhalechina/pumpkin-book>)（南瓜书是对西瓜书中公式的详细分析）

[南瓜书在线阅读](<https://datawhalechina.github.io/pumpkin-book/#/chapter3/chapter3>)

##### 学习过程

简单分成三个阶段学习,找实习期间努力完成一二阶段，第三阶段在学有余力刷Kaggle时再研究.

- 先看懂基本原理，整理推导公式，(主要就是西瓜书/NG课程/一些博客)
- 可以口述+手推主要原理，了解更复杂的模型，能够写出伪代码，能够说清楚各模型之间的优劣．
- 会调用sk-learn之类的库，会使用C++/python实现简单算法，会调参，会在常见任务上得到不错的结果，并且会使用可视化技术展示成果．

---



#### 模型之外的问题

- 归纳偏好，依赖某领域知识，例如奥卡姆剃刀。
- 没有免费午餐定理：没有最好的分类器，只有最合适的分类器．

##### 生成模型与判别模型

- 生成模型：获得P(X,Y)的联合概率分布，包括朴素贝叶斯，隐马尔可夫模型，求解时公式如下：
  $$
  P(Y|X)=\frac{P(X,Y)}{P(X)}
  $$

- 判别模型：直接获得条件概率分布P(Y|X)．统计学习方法中大部分方法都是判别模型．

##### 评估方法

测试集小，方差大；训练集小，偏差大

- 留出法
- 交叉验证法
- 自助法，在数据集少的时候用，会引入估计偏差。

##### 性能度量

- 分类问题：（在实际任务中往往更关注某一方面）

  |            | 预测为真 | 预测为假 |
  | :--------: | -------- | -------- |
  | 实际，为真 | TP       | FN       |
  | 实际，为假 | FP       | TN       |

  

  - 准确率：被正确分类样本占总样本的比例．acc = TP+TN/TP+FN+FP+TN;

  - 查准率：预测正确的正例与所有预测为证的数据之比．precision = TP/TP+FP;

  - 查全率：预测正确的正例与所有正例之比．recall = TP/TP+FN;

  - ＰＲ曲线：

    - 以ｐ为纵坐标ｒ为横坐标的曲线，若以个曲线包围了另一个曲线，则效果更好．
    - 按照样本为正的概率，从最大到最小进行划分．

  - 有时以pr曲线中，p=r的点为衡量标准，越大越好．

  - Ｆ１值：为准确率与召回率的调和均值：
    $$
    \frac2{F_1}=\frac1{P}+\frac1{R}
    $$

    $$
    \frac{1}{F_\beta}=\frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})
    $$

  - ROC：
    - 纵轴：真正例率：TPR  =TP/TP+FN（查全率）（真的中预测为真）．
    - 横轴：假正例率：FPR = FP/TN+FP．(假的中预测为真)．
    - AUC：ROC下方面积。

  - 代价曲线：

    - 对分类错误给一个准确的分类代价，前几种方法简单以出现次数为代价．
    - 代价敏感错误率
    - 代价曲线（不懂）

- 方差与偏差：是对过拟合与欠拟合的一种描述．**偏差**与**方差**分别是用于衡量一个模型**泛化误差**的两个方面；

  - 模型的**偏差**
    - 指的是模型预测的**期望值**与**真实值**之间的差；
    - 表示模型的拟合能力．体现在训练误差上．
    - 通常是由于模型不对导致偏差较大．
  - 模型的**方差**，指的是模型预测的**期望值**与**预测值**之间的差平方和；
    - 描述模型的稳定性．
    - 模型相对于数据复杂度太高导致．
    - 不同的训练样本对统一测试集的变化较大，属于过拟合．

##### 正则化

这种方法会基于模型复杂性对其进行惩罚，它喜欢相对简单能够更好的泛化的模型。

- L1,L2正则化为什么可以导致参数稀疏/变小.
  - 图示分析.
  - 梯度的角度.
  - 引入先验分布的角度解释

##### 二分类往多分类的推广

- OvO
  - 两两之间训练一个分类器，会有(n-1)n/2个分类器。
  - 投票法得最终结果
- OvR
  - 选取一类，剩余的为反例。
  - 选取为正例得那个，若有多个，则看预测置信度。
- MvM
  - 分成两大类，使用纠错输出码，编码，使得码间距最大
  - 选择码间距最小的作为类别判定。（海明距离或者欧氏距离）

##### 类别不平衡问题

- 阈值调整，原来概率大于0.5算是整理，现在提高一些，比如是1-p，p为正例占比。
  - 有个问题，这基于训练集是真实数据得无偏采样，采样可能并不完全服从数据集得分布。
- 欠采样
- 过采样：直接重复采样会导致过拟合。
  - SMOTE：选取同类样本得k近邻，随机选取一个，随机选取权值后，线性插值。
- 先训练判断时阈值移动。

##### 特征选择与稀疏学习(也蛮重要)

> 建议吧西瓜书中的概念都好好看看

---



#### 常用分类/回归/聚类模型

##### 线性模型

- 基于最小二乘法的线性回归，可得到最优闭式解。（公式推导）
- 引入联系函数后得到广义线性模型。
- 感知机：
  - 单层感知机类似对数几率回归，使用广义线性模型与signal函数推导出来的，但是signal函数不是连续可导，也不饿能给出概率分布，不太好用。
  - 多层感知机加上sigmoid输出、与交叉熵loss、与relu，就是神经网络雏形了。
- 对数几率回归（LR回归）：
  - 将回归问题转化为分类问题，是广义线性模型与对数几率函数的结合。
  - 极大似然法优化（公式推导）
  - 高阶可导连续凸函数，有x有ln有e，不一定有闭式解（这个我不太清楚）。
  - 优缺点：
    - 可以预测出近似概率。
    - 对率函数是任意阶可到凸函数，数学性质好，容易优化。
    - 是一个线性模型。
- 线性判别分析
  -  投影到一维直线上的公式推导。（公式推导，类内散度矩阵、类间散度矩阵）
  - 多维投影中的概念（推到、求解太复杂了）

##### 决策树

- 伪代码表示下树的生成。（**伪代码**）

- 优点：
  - 结构简单，速度快: 计算量相对较小, 且容易转化成分类规则，规则简单。
  - 可解释性: 挖掘出来的分类规则准确性高, 便于理解．
  - 可以处理连续值、无序属性、缺失值。
  - 不需要任何领域知识和参数假设，SVM需要挑选核函数．
  - 适合高维数据．
- 缺点
  - 容易过拟合，剪枝操作比较粗糙．（随机森林却巧妙得应用了这一点）
  - 忽略属性之间的相关性，单变量决策，每次都沿着垂直坐标轴的方向分割，用多变两联合学习开支很大．
- 要点：
  - 特征选择，提纯的过程：
    - 信息增益：经验熵减去条件经验熵，即：选择A为特征后，熵的下降，也就是不确定度的下降。
    - 信息增益比：信息增益与数据集D对特征A的值的熵之比。
      - c4.5中的启发式方法。
    - 基尼指数。
  - 剪枝
    - 通过极小化决策树整体的损失函数实现。
    - 分为预剪枝与后剪枝，判断在测试集上的泛化误差是否下降。
      - 预剪枝通过训练集生成树，每生成一个节点的时候，判断损失函数。预剪枝速度快，但可能因为剪掉了大树干而造成欠拟合。
      - 后剪枝在决策树生成之后此下而上，逐节点剪枝，计算量大但是一定程度上避免了欠拟合。
- ID3：
  - 简单的生成。
  - 只有生成算法，容易产生过拟合。
- C4.5/C5.0（同一方法，两个版本）
  - 生成部分，相比ID3，使用信息增益比替代信息熵。
  - 加入了剪枝策略。
  - 连续值与缺失值的处理。
- CART分类回归树
  - 回归树，最小二乘回归树．
  - 分类树，使用基尼指数．
  - 剪枝方法，用的是Breiman的递归方法剪枝，具体怎么实现还不太清楚．
- 连续值处理：
  - 二分法，连续属性可以渗透到子树中去（我理解为连续值取值太多，仅一次二分肯定不是很好用）
- 缺失值处理（对所有样本加权）
  - 某数据某属性缺失，基本上是不考虑这些数据，计算熵增益．
  - 划分样本过程中，C4.5中会把改数据的权重减小，再放入下一阶段的样本中．
- 多变量决策树（斜决策树）
  - 如果按照单属性分割数据的话，空间之间的间隔都是平行于属性坐标轴的，而多变量决策树则可以斜着分类，减少了树的深度．

##### 支持向量机（<font color = ff0000>很重要，必考</font>）

​	SVM试图用一个超平面将样本分成两个部分，这个超平面要求其到样本的最小距离最大，因为最小距离一般仅与少量样本点有关，这些样本就叫做支持向量，这种方法也就叫做支持向量机。有时候样本不是线性可分，引入了核函数与软间隔。

​	重中之重是核方法。SMO也很重要，没看细节。

- 由感知机发展而来：
  - 感知机是一种线性二分类问题，目的是使得误分类数据样本离超平面的总距离最短．
  - 使用梯度下降法求解．
  - 由于结果不唯一，需要加约束得到唯一结果，在此基础上发展出了支持向量机SVM
- 具体推导（公式推导）
  - 基本形式的由来（统计学习方法比较好）。
  - 虽是凸二次规划问题，但是用拉格朗日乘子法更快。
  - 虽然求解a仍然是二次规划问题，但是数据规模大的时候训练困难，就采用了启发式的SMO方法。
    - 选择两个距离相差最大变量，可以给目标函数值更大的变化,然后更新b值。
    - 如何找到两个a，可以参考[这篇博客](<https://www.cnblogs.com/pinard/p/6111471.html>)
  - b的求解，使用支持向量的平均值。
- 核函数，引入映射函数，将数据映射到高维空间，线性可分。
  - 无论是求解a、b还是对新的输入判别，都只包含输入之间的内积。省去了定义映射函数的过程，只需要高维空间的内积公式就可以。
  - 核函数的判定：任何一个对称函数对应的核矩阵是半正定矩阵（对应任意一组变量的GRAM矩阵），就可以作为核函数。数学定义不清楚，大约就是线性核、多项式核、高斯核、拉普拉斯核、sigmoid核。
- 软间隔与正则化
  - 引入松弛变量，相当于引入经验风险hinge损失，由于其有平坦区域，导致仍然存在稀疏的支持向量。（0<a<c）
  - 还有其他的损失函数，如指数损失，对率损失，但是这些函数是链续可导凸函数，结果没有稀疏性，计算开销较大。（支持向量机中，大部分a==0）
- 支持向量回归
  - 与传统的线性回归不同，SVR中的样本只要落在一定范围内就算做正确。相当于在普通回归上修改了经验风险，加上了L2正则。
- 核方法（真正的重点）
  - 如何判定一个函数是核函数？
  - 表示定理，可以将很多线性方法转化为非线性方法。
- 优缺点
  - 只能用于二分类问题
  - 训练时空代价比较高，需要存储核矩阵。样本量大的时候计算复杂的高（有了SMO快一点）
  - 需要选取合适的核函数，需要调试超参数C。
  - 泛化能力很强，依赖少量关键数据，降低了对数据分布与数据规模的要求，比较稳定，是一种很好的小样本学习方法。
  - 只依赖部分支持向量，测试过程比较简单快速。

##### KNN近邻

基于实例的方法，训练复杂度为零，懒惰学习的代表

- 由距离最近的ｋ个点决定分类类别．
- 根据不同的ｋ值，空间会被分成不同的块．
- Lp距离度量．
- 实现：KD树．如果存下两两之间的距离，这个内积矩阵(GRAM matrix)太大．需要使用特殊结构存储．
  - 按中位数把数据分成一棵平衡二叉树．
  - 最近邻搜索
    - 深入叶结点，得最短距离ｄ．
    - 返回父结点，比较距离．并检查另一个区域是否与球形空间有重合．
    - 若有重合，则对此结点递归调用最近邻搜索．
    - 若无重合，则继续网商，返回父结点，即回到第二步．
  - 可以将ｋ个最小值存进最大堆，从而得到ｋ个最近邻值．
- kd树平均搜索复杂度为log(N)，kd树适合训练样本远大于空间维度的情况．不然效率会极大降低至线性扫描．

#####　贝叶斯方法

​	明确应用了贝叶斯定理来解决如分类和回归等问题的方法。用于属性间关系不大的情况，可用于垃圾邮件回收，快速、易于训练、给出了它们所需的资源能带来良好的表现，如果输入变量是相关的，则会出现问题．

- 贝叶斯决策理论（公式推导）
- 极大似然法（公式推导）
  - 严重依赖假设

- Naive Bayes，朴素贝叶斯．（具体公式见机器学习）
  - 基于贝叶斯定理与特征条件独立假设．
  - 加入拉普拉斯修正，防止概率为零．
- 半朴素贝叶斯：假设每一个属性仅与另一个属性相关．（没看懂）
- 贝叶斯信念网络：需要优化网络．（没看懂）
- EM算法：处理数据中有隐变量的情况．

##### 集成方法（<font color = ff0000>很重要，必考</font>）

具有多样性与准确性的多个学习器集成，会有更好的效果。

boosting：学习器之间相互依赖，需要串行生成的序列化方法。Adaboost：降低偏差。

bagging：学习器之间不存在强依赖关系，可并行执行。RF：降低方差。

怎么说呢，很复杂建议多看看西瓜书后面的推荐，[Zhou2012]专门讲了集成学习。

[一篇还不错的博客，他的主页里还有很多算法讲解](<https://www.cnblogs.com/pinard/p/6140514.html>)

- Adaboost
  - 是一种典型的二分类问题，加法模型+前向分部模型+指数损失，来处理二分类问题。
  - （公式推导）
  - 0/1损失与指数损失为一致性损失函数。
  - 每个学习器的衰减系数相当于正则化？通过对样本加。
- GBDT
  - 首先是回归问题，用平方误差损失，分类就用指数损失或者是对数损失。
  - 平方损失就是拟合残差。
  - 一般损失，先拟合梯度，再在新树的每个区间内计算最佳拟合值。
  - 分类问题：使用指数损失则退化为Adaboost，使用对数损失可用于多分类问题（交叉熵）。
  - 正则化，给新树加上一个权重/降低采样比例。
  - 优缺点：
    - 可以灵活处理各种类型的数据，包括连续值和离散值。也可以改为回归、分类、多分类。
    - 在相对少的调参时间情况下，预测的准确率也可以比较高。
    - 使用一些健壮的损失函数，对异常值的鲁棒性非常强。（集成学习的一个特点）
    - 由于弱学习器之间存在依赖关系，难以并行训练数据
- Xgboost
  - 对GDBT的改进，包括使用二阶导数。显示加上正则化项。内存上的一些优化。

- bagging（以随机森林为例）
  - 基于决策树，简单有效，是一种集成学习方法．
  - 重点降低方差，对易受样本扰动的学习器效果较好。（Dropout有点bagging的意思）
  - 使用自主采样法，分配样本。
- 随机森林
  - 分配属性还引入了对属性集的随机选择，更加多样性。
  - 它能够处理很高维度（feature很多）的数据，并且不用做特征选择
  - 在训练完后，它能够给出哪些feature比较重要
  - 泛化能力强
  - 训练速度快，容易做成并行化方法
  -  实现比较简单
  - 很好地处理缺失值
  - 数据噪声大的时候会过拟合。


##### 聚类算法

> 聚类算法是指对一组目标进行分类，属于同一组（亦即一个类，cluster）的目标被划分在一组中，与其他组目标相比，同一组目标更加彼此相似（在某种意义上）。让数据变得有意义，结果难以解读，针对不寻常的数据组，结果可能无用。
>
> 聚类算法在实践中很常用,变化也很多,目前并没有系统的教材.

两个关键问题，性能度量/距离计算．

- 性能度量
  - 簇间相似度/簇内相似度
- 距离计算
  - 闵可夫斯基距离系列.
  - 一般来说,距离度量有四个性质:非负性/同一性/对称性/直递性(三角不等式)

- 原型聚类

  此算法假设聚类结构可以通过一组原型刻画

  - k--means:(伪代码): 选取距离最近向量作为归属类,后对同一类中向量取平均值,更新该类的中心向量.
  - LVQ:带有标记的样本
    - 选取带标记的q个中心向量,随机选取一个样本,计算距离最近的中心向量.
    - 若类别相同则将中心向量靠近x,否则远离.
    - 直到满足终止条件.
  - 密度聚类
    - 核心对象/密度可达/密度直达,没搞清楚.

---



#### 没咋用过的模型

##### 降维算法

- 主成分分析（PCA）．

  从最近重构性(样本离平面距离都很近)/最大可分性(样本之间尽可能分开)可推导出同一个降维模型
  $$
  \max tr(W^TXX^TW) /s.t. W^TW=I
  $$

  - 样本中心化
  - 计算样本协方差$XX^T​$
  - 对协方差矩阵$XX^T$做特征值分解
  - 取最大的几个特征值对应的特征向量.

- 判别分析．对多个类映射,保证类内距最小,类间距最大.

- 核化PCA

  使用核方法,进行非线性降维

- 流形学习

- 度量学习,学得一种合适的距离度量.

##### 半监督学习

- 主动学习:查询不确定性较大的数据.
- 半监督主要利用聚类假设/流形假设.
- 半监督SVM(TSVM)
  - 先指派标记,训练,再选出分类错误最大的两个,交换标记,继续训练.
- 基于分歧的方法(!比较重要)
  - 相互学习共同进步.
  - 选择两个充分且条件独立的视图,训练两个分类器,两个分类器把各自置信度较高的无监督样本,打上标记给对方学习.是一种与集成学习结合的方法.

##### 最大熵模型

​	最大熵模型本身不是分类器，它一般是用来判断模型预测结果的好坏的。LR其实就是使用最大熵模型作为优化目标的一个算法．

##### 隐马尔可夫

​	这是一个基于序列的预测方法，核心思想就是通过上一个（或几个）状态预测下一个状态。之所以叫“隐”马尔科夫是因为它的设定是状态本身我们是看不到的，我们只能根据状态生成的结果序列来学习可能的状态。

##### 概率图模型

​	图模型或概率图模型（PGM/probabilistic graphical model）是一种概率模型，一个图（graph）可以通过其表示随机变量之间的条件依赖结构（conditional dependence structure）。模型清晰易理解，确定其依赖的拓扑很困难，有时候也很模糊．

- 贝叶斯网络
- 马尔可夫随机场
- 链图

---



#### 可能被问到的一些问题

##### 你觉得什么方法最好？

- 没有最好的方法，只有最合适的方法。
- 总的来看，集成方法最好，用单一模型，很可能非线性不够或者是特征相关性不够。
- 如果能用好深度学习还挺不错，没试过。目前接触的深度学习，处理的数据比较规整，情况不像机器学子这么复杂。

##### 如何防止过拟合？

由于学习能力太强导致。	

- 引入结构风险：参数正则化（奥卡姆剃刀，减少模型自由度），决策树的剪枝
- 提前停止
- 数据增强
- BN层（可能是让数据分布相同，使局部特征不明显）
  - 均值归一化，使得各个维度上分布近似相同，一定程度上防止梯度下降时的抖动．[这里有图解](https://www.cnblogs.com/nolonely/p/6184196.html)
- dropout，类似集成学习？

##### 各种距离的描述

- 欧式距离，曼哈顿距离（L_P距离），（闵可夫斯基距离）
  $$
  L_p=(\sum|x_i^l-x_j^l|^p)^{\frac{1}{p}}
  $$
  p=1,曼哈顿距离．

  p=2,欧式距离

  p=正无穷，最大坐标差．切比雪夫距离．国际象棋中王移动到某个位置的最少步数．

- 海明距离

  两个编码之间的距离，码距．

- 马氏距离

  一个分布内，两个样本之间的距离．中间是分布的协方差矩阵．
  $$
  d_M(x,y)=\sqrt{(x-y)^T(\sum)^{-1}(x-y)}
  $$

- 余弦距离

  两个向量夹角的余弦值．

##### 有哪些优化方法?

- 梯度下降法系列
  - 原始版本随机梯度下降法：沿着梯度反方向前进．（使用全体样本更新）
  - 随机梯度下降，每次选取小批量样本下降．
  - 牛顿法，加入海森矩阵优化梯度，用到了二阶的傅立叶展开．

---

#### 其他

##### 伪代码格式

```python
repeat
	xxx
until xxx

if xxx then
	xxx
else
	xxx
end if


```

---



#### 具体领域常用模型

##### 风控

##### 推荐系统

##### 搜索

这是行内公式：$\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N​$