#### 前言

##### 简单总结

机器学习覆盖范围很广，历史悠久，没有时间系统学习了，要有针对性地了解某些方法．比如面试常考SVM\随机森林，就多看一些；比如有的公司做推荐系统/有的做风控/有的做搜索，就需要具体了解一下涉及到哪些知识点．

主要还是把西瓜树过一遍．西瓜书上有很多实用过程中可能会遇到的问题,也有很多思路由来的解释,具体的算法也给出了引用,是一本非常好的入门教材.

##### 这里是一些参考与经验总结

[CITE1_XYZH的文章](https://www.zhihu.com/question/26726794/answer/151282052)

[机器之心](https://zhuanlan.zhihu.com/p/25327755)

##### 学习过程

简单分成三个阶段学习,找实习期间努力完成一二阶段，第三阶段在学有余力刷Kaggle时再研究.

- 先看懂基本原理，整理推导公式，(主要就是西瓜书/NG课程/一些博客)
- 可以口述+手推主要原理，了解更复杂的模型，能够写出伪代码，能够说清楚各模型之间的优劣．
- 会调用sklearn之类的库，会使用C++/python实现简单算法，会调参，会在常见任务上得到不错的结果，并且会使用可视化技术展示成果．

---



#### 模型之外的问题

没有最好的分类器，只有最合适的分类器．

##### 生成模型与判别模型

- 生成模型：获得P(X,Y)的联合概率分布，包括朴素贝叶斯，隐马尔可夫模型，求解时公式如下：
  $$
  P(Y|X)=\frac{P(X,Y)}{P(X)}
  $$

- 判别模型：直接获得条件概率分布P(Y|X)．统计学习方法中大部分方法都是判别模型．

##### 性能度量

- 分类问题：（在实际任务中往往更关注某一方面）

  |            | 预测为真 | 预测为假 |
  | :--------: | -------- | -------- |
  | 实际，为真 | TP       | FN       |
  | 实际，为假 | FP       | TN       |

  

  - 准确率：被正确分类样本占总样本的比例．acc = TP+TN/TP+FN+FP+TN;

  - 查准率：预测正确的正例与所有预测为证的数据之比．precision = TP/TP+FP;

  - 查全率：预测正确的正例与所有正例之比．recall = TP/TP+FN;

  - ＰＲ曲线：

    - 以ｐ为纵坐标ｒ为横坐标的曲线，若以个曲线包围了另一个曲线，则效果更好．
    - 按照样本为正的概率，从最大到最小进行划分．

  - 有时以ｐｒ曲线中，p=r的点为衡量标准，越大越好．

  - Ｆ１值：为准确率与召回率的调和均值：
    $$
    \frac2{F_1}=\frac1{P}+\frac1{R}
    $$

  - ROC：
    - 纵轴：真正例率：TPR  =TP/TP+FN（查全率）（真的中预测为真）．
    - 横轴：假正例率：FPR = FP/TN+FP．(假的中预测为真)．
  - 代价曲线：

    - 对分类错误给一个准确的分类代价，前几种方法简单以出现次数为代价．

- 回归问题：最小二乘法求解．

- 方差与偏差：是对过拟合与欠拟合的一种描述．**偏差**与**方差**分别是用于衡量一个模型**泛化误差**的两个方面；

  - 模型的**偏差**
    - 指的是模型预测的**期望值**与**真实值**之间的差；
    - 表示模型的拟合能力．体现在训练误差上．
    - 通常是由于模型不对导致偏差较大．
  - 模型的**方差**，指的是模型预测的**期望值**与**预测值**之间的差平方和；
    - 描述模型的稳定性．
    - 模型相对于数据复杂度太高导致．
    - 不同的训练样本对统一测试集的变化较大，属于过拟合．

##### 正则化

它是另一种方法（通常是回归方法）的拓展，这种方法会基于模型复杂性对其进行惩罚，它喜欢相对简单能够更好的泛化的模型。

- L1,L2正则化为什么可以导致参数稀疏/变小.
  - 图示分析.
  - 梯度的角度.
  - 引入先验分布的角度

---



#### 常用分类/回归/聚类模型

##### 支持向量机

<font color = ff0000>很重要，必考</font>

​	SVM 模型将训练事例表示为空间中的点，它们被映射到一幅图中，由一条明确的、尽可能宽的间隔分开以区分两个类别。随后，新的示例会被映射到同一空间中，并基于它们落在间隔的哪一侧来预测它属于的类别．

​	在非线性可分问题上表现优秀，非常难以训练？？

- 由感知机发展而来：
  - 感知机是一种线性二分类问题，目的是使得误分类数据样本离超平面的总距离最短．
  - 使用梯度下降法求解．
  - 由于结果不唯一，需要加约束得到唯一结果，在此基础上发展出了支持向量机SVM

- 具体推导见统计机器学习
- 使用核技巧，能够处理各种数据，抗干扰性能好，在文本分类上效果好（文本维度高？）



##### KNN近邻

基于实例的方法,模型复杂度为O(1),懒惰学习.

- 由距离最近的ｋ个点决定分类类别．
- 根据不同的ｋ值，空间会被分成不同的块．
- Lp距离度量．
- 实现：KD树．如果存下两两之间的距离，这个内积矩阵(GRAM matrix)太大．需要使用特殊结构存储．
  - 按中位数把数据分成一棵平衡二叉树．
  - 最近邻搜索
    - 深入叶结点，得最短距离ｄ．
    - 返回父结点，比较距离．并检查另一个区域是否与球形空间有重合．
    - 若有重合，则对此结点递归调用最近邻搜索．
    - 若无重合，则继续网商，返回父结点，即回到第二步．
- kd树平均搜索复杂度为log(N)，kd树适合训练样本远大于空间维度的情况．不然效率会极大降低至线性扫描．



#####　贝叶斯方法

​	明确应用了贝叶斯定理来解决如分类和回归等问题的方法。用于属性间关系不大的情况，可用于垃圾邮件回收，快速、易于训练、给出了它们所需的资源能带来良好的表现，如果输入变量是相关的，则会出现问题．

- Naive Bayes，朴素贝叶斯．（具体公式见机器学习）
  - 基于贝叶斯定理与特征条件独立假设．
  - 加入拉普拉斯修正，防止概率为零．
- 高斯朴素贝叶斯：没看到
- 半朴素贝叶斯：假设每一个属性仅与另一个属性相关．（没看懂）
- 贝叶斯信念网络：需要优化网络．（没看懂）
- EM算法：处理数据中有隐变量的情况．

##### 决策树

​	能够生成清晰的基于特征的树状结构，简单有效，是很多算法的基础，例如随机森林，提升树，但是容易被攻击．内容有可读性，分类快速。

- 优点：
  - 速度快: 计算量相对较小, 且容易转化成分类规则.，规则简单．
  - 可解释性: 挖掘出来的分类规则准确性高, 便于理解．
  - 可以处理连续和其他字段．
  - 不需要任何领域知识和参数假设，SVM需要挑选核函数．
  - 适合高维数据．
- 缺点
  - 容易过拟合，剪枝操作比较粗糙．（随机森林却巧妙得应用了这一点）
  - 忽略属性之间的相关性，单变量决策，每次都沿着垂直坐标轴的方向分割，用多变两联合学习开支很大．

- 分成三个步骤：
  - 特征选择：
    - 信息增益：经验熵减去条件经验熵，即：选择A为特征后，熵的下降，也就是不确定度的下降。
    - 信息增益比：信息增益与数据集D对特征A的值的熵之比。
  - 决策树生成
    - 根据信息增益是否超过某一个定值来分类，每次数据集会少一些，特征也少一个，直到分类结束。
  - 剪枝
    - 通过极小化决策树整体的损失函数实现。
    - 分为预剪枝与后剪枝
      - 预剪枝通过训练集生成树，每生成一个节点的时候，判断损失函数。预剪枝速度快，但可能因为剪掉了大树干而造成欠拟合。
      - 后剪枝在决策树生成之后此下而上，逐节点剪枝，计算量大但是一定程度上避免了欠拟合。
- ID3：
  - 简单的生成。
  - 只有生成算法，容易产生过拟合。
- C4.5/C5.0（同一方法，两个版本）
  - 生成部分，相比ID3，使用信息增益比替代信息熵。
  - 加入了剪枝策略。
- CART分类回归树
  - 回归树，最小二乘回归树．
  - 分类树，使用基尼指数．
  - 剪枝方法，用的是Breiman的递归方法剪枝，具体怎么实现还不太清楚．
- 连续值处理：
  - 选取中间值划分
- 缺失值处理
  - 若是训练过程中，某数据某属性缺失，基本上是不考虑这些数据，计算熵增益．
  - 若是测试过程中，测试样例某属性缺失，C4.5中会把改数据的权重减小，再放入下一阶段的样本中．
- 多变量决策树（斜决策树）
  - 如果按照单属性分割数据的话，空间之间的间隔都是平行于属性坐标轴的，而多变量决策树则可以斜着分类，减少了树的深度．

##### 对率回归

​	

##### 神经网络

​	需要大量数据进行训练；训练要求很高的硬件配置；模型处于「黑箱状态」，难以理解内部机制；元参数（Metaparameter）与网络拓扑选择困难。

- 感知机
- 深度学习

##### 集成方法（ensemble）

<font color = ff0000>很重要，必考</font>

​	集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。当先最先进的预测几乎都使用了算法集成。它比使用单个模型预测出来的结果要精确的多.

- 提升算法（boosting）
  - Adaboost思想，典型如提升树
  - 梯度提升回归树（Gradient Boosted Regression Trees，GBRT）
  - Xgboost是GB算法的高效实现
- bagging

- 随机森林
  - 基于决策树，简单有效，是一种集成学习方法．

##### 聚类算法

​	聚类算法是指对一组目标进行分类，属于同一组（亦即一个类，cluster）的目标被划分在一组中，与其他组目标相比，同一组目标更加彼此相似（在某种意义上）。让数据变得有意义，结果难以解读，针对不寻常的数据组，结果可能无用。

- K-means

---



#### 没咋用过的模型

##### 降维算法

​	和集簇方法类似，降维追求并利用数据的内在结构，目的在于使用较少的信息总结或描述数据。可处理大规模数据集，难以搞定非线性数据．

- 主成分分析（PCA）．
- 判别分析．

##### 最大熵模型

​	最大熵模型本身不是分类器，它一般是用来判断模型预测结果的好坏的。LR其实就是使用最大熵模型作为优化目标的一个算法．

##### 隐马尔可夫

​	这是一个基于序列的预测方法，核心思想就是通过上一个（或几个）状态预测下一个状态。之所以叫“隐”马尔科夫是因为它的设定是状态本身我们是看不到的，我们只能根据状态生成的结果序列来学习可能的状态。

##### 概率图模型

​	图模型或概率图模型（PGM/probabilistic graphical model）是一种概率模型，一个图（graph）可以通过其表示随机变量之间的条件依赖结构（conditional dependence structure）。模型清晰易理解，确定其依赖的拓扑很困难，有时候也很模糊．

- 贝叶斯网络
- 马尔可夫随机场
- 链图

---



#### 可能被问到的一些问题

##### 如何防止过拟合？

- 参数正则化（奥卡姆剃刀，减少模型自由度）
- 提前停止
- 数据增强
- BN层（可能是让数据分布相同，使局部特征不明显）
  - 均值归一化，使得各个维度上分布近似相同，一定程度上防止梯度下降时的抖动．[这里有图解](https://www.cnblogs.com/nolonely/p/6184196.html)
- dropout，类似集成学习？
- 决策树的剪枝操作

##### 类别不平衡如何解决?

- 欠采样
- 过采样，但需要特殊的过采样方法（SMOTE）
- 阈值移动
- 加上不同的损失权重

##### 各种距离的描述

- 欧式距离，曼哈顿距离（L_P距离），（闵可夫斯基距离）
  $$
  L_p=(\sum|x_i^l-x_j^l|^p)^{\frac{1}{p}}
  $$
  p=1,曼哈顿距离．

  p=2,欧式距离

  p=正无穷，最大坐标差．切比雪夫距离．国际象棋中王移动到某个位置的最少步数．

- 海明距离

  两个编码之间的距离，码距．

- 马氏距离

  一个分布内，两个样本之间的距离．中间是分布的协方差矩阵．
  $$
  d_M(x,y)=\sqrt{(x-y)^T(\sum)^{-1}(x-y)}
  $$

- 余弦距离

  两个向量夹角的余弦值．

##### 有哪些优化方法?

###### 梯度下降法系列

- 原始版本随机梯度下降法：沿着梯度反方向前进．（使用全体样本更新）
- 随机梯度下降，每次选取小批量样本下降．
- 牛顿法，加入海森矩阵优化梯度，用到了二阶的傅立叶展开．

---



#### 具体领域常用模型

##### 风控

##### 推荐系统

##### 搜索

